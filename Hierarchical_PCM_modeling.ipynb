{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IE57LHLN3EJ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "SR = 44100\n",
        "NOISE = np.random.rand(SR * 10).astype(np.float32)\n",
        "t = np.arange(SR * 10).astype(np.float32) / SR\n",
        "SINE = np.sin(2.0 * np.pi * t * 440.0)\n",
        "EXTREME = np.array([-1.0, 0.0, 1.0], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "vQsQSkWN3WKc"
      },
      "outputs": [],
      "source": [
        "def quantize_unsigned_pcm(x: np.ndarray, n_bits: int) -> np.ndarray:\n",
        "    if x.dtype != np.float32:\n",
        "        raise ValueError(\"x must be float32\")\n",
        "    if x.min() < -1 or x.max() > 1:\n",
        "        raise ValueError(\"x must be in range [-1, 1]\")\n",
        "    if n_bits < 1 or n_bits > 64:\n",
        "        raise ValueError(\"n_bits must be between 1 and 64\")\n",
        "\n",
        "    # Map from [-1, 1] to [0, 1)\n",
        "    x_normalized = (x + 1) / 2\n",
        "\n",
        "    # Scale by 2^n_bits (not 2^n_bits - 1) to maintain MSB invariance\n",
        "    scale = 2 ** n_bits\n",
        "    x_scaled = x_normalized * scale\n",
        "\n",
        "    # Use floor to convert to integer (not round, to maintain MSB invariance)\n",
        "    x_floored = np.floor(x_scaled)\n",
        "\n",
        "    # Clamp to valid range [0, 2^n_bits - 1]\n",
        "    max_val = (2 ** n_bits) - 1\n",
        "    x_clamped = np.clip(x_floored, 0, max_val)\n",
        "\n",
        "    return x_clamped.astype(np.uint64)\n",
        "\n",
        "\n",
        "def msb(x: np.ndarray, orig_n_bits: int, n_bits: int) -> np.ndarray:\n",
        "    if x.dtype != np.uint64:\n",
        "        raise ValueError(\"x must be uint64\")\n",
        "    return (x >> (orig_n_bits - n_bits)) & ((1 << n_bits) - 1)\n",
        "\n",
        "def lsb(x: np.ndarray, n_bits: int) -> np.ndarray:\n",
        "    if x.dtype != np.uint64:\n",
        "        raise ValueError(\"x must be uint64\")\n",
        "    return x & ((1 << n_bits) - 1)\n",
        "\n",
        "for x in [NOISE, SINE, EXTREME]:\n",
        "    x_24b = quantize_unsigned_pcm(x, 24)\n",
        "    x_16b = quantize_unsigned_pcm(x, 16)\n",
        "    x_13b = quantize_unsigned_pcm(x, 13)\n",
        "    x_8b = quantize_unsigned_pcm(x, 8)\n",
        "\n",
        "    #x_24b_16msb = (x_24b >> (24 - 16)) & 0xFFFF\n",
        "    x_24b_16msb = msb(x_24b, 24, 16)\n",
        "    assert np.array_equal(x_24b_16msb, x_16b)\n",
        "\n",
        "    #x_24b_13msb = (x_24b >> (24 - 13)) & 0x1FFF\n",
        "    x_24b_13msb = msb(x_24b, 24, 13)\n",
        "    assert np.array_equal(x_24b_13msb, x_13b)\n",
        "\n",
        "    #x_16b_8msb = (x_16b >> (16 - 8)) & 0xFF\n",
        "    x_16b_8msb = msb(x_16b, 16, 8)\n",
        "    assert np.array_equal(x_16b_8msb, x_8b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "def quantize_unsigned_pcm_torch(x: torch.tensor, n_bits: int) -> torch.tensor:\n",
        "    if x.dtype != torch.float32:\n",
        "        raise ValueError(\"x must be float32\")\n",
        "    if x.min() < -1 or x.max() > 1:\n",
        "        raise ValueError(\"x must be in range [-1, 1]\")\n",
        "    if n_bits < 1 or n_bits > 64:\n",
        "        raise ValueError(\"n_bits must be between 1 and 64\")\n",
        "\n",
        "    # Map from [-1, 1] to [0, 1)\n",
        "    x_normalized = (x + 1) / 2\n",
        "\n",
        "    # Scale by 2^n_bits (not 2^n_bits - 1) to maintain MSB invariance\n",
        "    scale = 2 ** n_bits\n",
        "    x_scaled = x_normalized * scale\n",
        "\n",
        "    # Use floor to convert to integer (not round, to maintain MSB invariance)\n",
        "    x_floored = torch.floor(x_scaled)\n",
        "\n",
        "    # Clamp to valid range [0, 2^n_bits - 1]\n",
        "    max_val = (2 ** n_bits) - 1\n",
        "    x_clamped = torch.clip(x_floored, 0, max_val)\n",
        "\n",
        "    return x_clamped.to(torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "def msb_torch(x: torch.tensor, orig_n_bits: int, n_bits: int) -> torch.tensor:\n",
        "    # if x.dtype != torch.uint64:\n",
        "    #     raise ValueError(\"x must be uint64\")\n",
        "    return (x >> (orig_n_bits - n_bits)) & ((1 << n_bits) - 1)\n",
        "\n",
        "def lsb_torch(x: torch.tensor, n_bits: int) -> torch.tensor:\n",
        "    # if x.dtype != torch.uint64:\n",
        "    #     raise ValueError(\"x must be uint64\")\n",
        "    return x & ((1 << n_bits) - 1)\n",
        "\n",
        "for x in [torch.from_numpy(NOISE), torch.from_numpy(SINE), torch.from_numpy(EXTREME)]:\n",
        "    x_24b = quantize_unsigned_pcm_torch(x, 24)\n",
        "    x_16b = quantize_unsigned_pcm_torch(x, 16)\n",
        "    x_13b = quantize_unsigned_pcm_torch(x, 13)\n",
        "    x_8b = quantize_unsigned_pcm_torch(x, 8)\n",
        "\n",
        "    #x_24b_16msb = (x_24b >> (24 - 16)) & 0xFFFF\n",
        "    x_24b_16msb = msb_torch(x_24b, 24, 16)\n",
        "    assert torch.equal(x_24b_16msb, x_16b)\n",
        "\n",
        "    #x_24b_13msb = (x_24b >> (24 - 13)) & 0x1FFF\n",
        "    x_24b_13msb = msb_torch(x_24b, 24, 13)\n",
        "    assert torch.equal(x_24b_13msb, x_13b)\n",
        "\n",
        "    #x_16b_8msb = (x_16b >> (16 - 8)) & 0xFF\n",
        "    x_16b_8msb = msb_torch(x_16b, 16, 8)\n",
        "    assert torch.equal(x_16b_8msb, x_8b)\n",
        "\n",
        "    # get middle 8 bits of 24b, should to 8 lsb of 16b\n",
        "    x_24b_8mid = lsb_torch(msb_torch(x_24b, 24, 16), 8)\n",
        "    assert torch.equal(x_24b_8mid, lsb_torch(x_16b, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((1 << 8) - 1) == 0xFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "class MonoWavChunkDataset(Dataset):\n",
        "    def __init__(self, data_dir, chunk_size=4096, sample_rate=44100, bit_split=False, epoch_expansion_factor=10, only_lower_bits=False, stereo_interleave=False):\n",
        "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
        "        self.chunk_size = chunk_size\n",
        "        self.sample_rate = sample_rate\n",
        "        self.bit_split = bit_split\n",
        "        self.epoch_expansion_factor = epoch_expansion_factor\n",
        "        self.only_lower_bits = only_lower_bits\n",
        "        self.stereo_interleave = stereo_interleave\n",
        "\n",
        "        if len(self.files) == 0:\n",
        "            raise ValueError(\"files is empty\")\n",
        "        print(f\"MonoWavChunkDataset: {len(self.files)} files, chunk_size={self.chunk_size}\")\n",
        "        pth = 'musdbstereo_lengths_train.json' if 'train' in data_dir else 'musdbstereo_lengths_valid.json' if 'valid' in data_dir else 'musdbstereo_lengths.json'\n",
        "        lengths = json.load(open(pth, 'r'))\n",
        "        for ix, f in enumerate(tqdm(self.files)):\n",
        "            self.files[ix] = (f, lengths[os.path.basename(f)])  # (path, num_samples)\n",
        "        self.files = self.files * self.epoch_expansion_factor\n",
        "        random.shuffle(self.files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, file_length = self.files[idx]\n",
        "        print(path)\n",
        "        # randomly sample a chunk of chunk_size from the file\n",
        "        chunk_size = self.chunk_size + 1\n",
        "        # offset = torch.randint(0, max(1, file_length - chunk_size), (1,)).item()\n",
        "        offset = 0\n",
        "        wav, sr = torchaudio.load(path, normalize=True, frame_offset=offset, num_frames=chunk_size, backend=\"soundfile\")\n",
        "        print(wav.shape, sr, wav.dtype, wav.min(), wav.max())\n",
        "        # wav, sr = torchaudio.load(path, normalize=False)\n",
        "        if wav.dtype != torch.int16:\n",
        "            wav = linear_encode(wav, bits=16)\n",
        "        else:\n",
        "            wav = wav.long() + 32768  # \n",
        "        print(wav.dtype, wav.min(), wav.max())\n",
        "        # randomly sample left or right channel\n",
        "        if self.stereo_interleave:\n",
        "            # put left, then right or right then left\n",
        "            interleaved = torch.zeros(wav.shape[1] * 2, dtype=wav.dtype)\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                interleaved[:wav.shape[1]] = wav[0]\n",
        "                interleaved[wav.shape[1]:] = wav[1]\n",
        "            else:\n",
        "                interleaved[:wav.shape[1]] = wav[1]\n",
        "                interleaved[wav.shape[1]:] = wav[0]\n",
        "            wav = interleaved\n",
        "        else:\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                wav = wav[1]  # take right channel only\n",
        "            else:\n",
        "                wav = wav[0]  # take left channel only\n",
        "        # if bit_split is set, split each 16-bit value into two 8-bit values representing the high and low bytes\n",
        "        if self.bit_split:\n",
        "            splits = self.bit_split if type(self.bit_split) is int else 2\n",
        "            if splits == 2:\n",
        "                high_bits = (wav >> 8) & 0xFF\n",
        "                low_bits = wav & 0xFF\n",
        "                # add 2^8 to the low bits to distinguish them from high bits\n",
        "                low_bits += 256\n",
        "                # interleave high and low bits\n",
        "                wav = torch.stack([high_bits, low_bits], dim=1).view(-1)\n",
        "                assert torch.all(wav[0] == high_bits[0])\n",
        "                assert torch.all(wav[1] == low_bits[0])\n",
        "\n",
        "                # split back into separate channels and depths for testing\n",
        "                high_bits_recon = wav[::2]\n",
        "                low_bits_recon = wav[1::2] - 256\n",
        "                channel1_hb = high_bits_recon[:high_bits_recon.shape[0]//2]\n",
        "                channel2_hb = high_bits_recon[high_bits_recon.shape[0]//2:]\n",
        "                channel1_lb = low_bits_recon[:low_bits_recon.shape[0]//2]\n",
        "                channel2_lb = low_bits_recon[low_bits_recon.shape[0]//2:]\n",
        "                return channel1_hb, channel1_lb, channel2_hb, channel2_lb \n",
        "\n",
        "            elif splits == 4:\n",
        "                byte3 = (wav >> 12) & 0x0F\n",
        "                byte2 = (wav >> 8) & 0x0F\n",
        "                byte1 = (wav >> 4) & 0x0F\n",
        "                byte0 = wav & 0x0F\n",
        "                # add 2^4, 2^8, 2^12 to distinguish them\n",
        "                byte2 += 16\n",
        "                byte1 += 32\n",
        "                byte0 += 48\n",
        "                wav = torch.stack([byte3, byte2, byte1, byte0], dim=1).view(-1)\n",
        "                assert torch.all(wav[0] == byte3[0])\n",
        "                assert torch.all(wav[1] == byte2[0])\n",
        "                assert torch.all(wav[2] == byte1[0])\n",
        "                assert torch.all(wav[3] == byte0[0])\n",
        "            elif splits == 3:\n",
        "                # first highest 8 bits, then next 4 bits, then lowest 4 bits\n",
        "                byte2 = (wav >> 8) & 0xFF\n",
        "                byte1 = (wav >> 4) & 0x0F\n",
        "                byte0 = wav & 0x0F\n",
        "                byte1 += 256\n",
        "                byte0 += 272\n",
        "                wav = torch.stack([byte2, byte1, byte0], dim=1).view(-1)\n",
        "                assert torch.all(wav[0] == byte2[0])\n",
        "                assert torch.all(wav[1] == byte1[0])\n",
        "                assert torch.all(wav[2] == byte0[0])\n",
        "        elif self.only_lower_bits:\n",
        "            wav = wav & 0xFF  # keep only the lower 8 bits\n",
        "\n",
        "\n",
        "        if sr != self.sample_rate:\n",
        "            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)\n",
        "        if len(wav) < self.chunk_size+1:\n",
        "            wav = torch.nn.functional.pad(wav, (0, self.chunk_size+1 - len(wav)), mode='constant', value=q_zero(bits=16))\n",
        "        chunk = wav\n",
        "        tokens = chunk.long()\n",
        "        seq_len = self.chunk_size\n",
        "        if self.bit_split:\n",
        "            seq_len *= self.bit_split if type(self.bit_split) is int else 2\n",
        "        if self.stereo_interleave:\n",
        "            seq_len *= 2\n",
        "        seq_len = min(seq_len + 1, len(tokens))\n",
        "        tokens = tokens[:seq_len]\n",
        "        input_tokens = tokens[:-1]\n",
        "        target_tokens = tokens[1:]\n",
        "        return input_tokens, target_tokens\n",
        "    \n",
        "\n",
        "def minmax_scale(tensor, range_min=0, range_max=1):\n",
        "    \"\"\"\n",
        "    Min-max scaling to [0, 1].\n",
        "    \"\"\"\n",
        "    min_val = torch.amin(tensor, dim=(1, 2), keepdim=True)\n",
        "    max_val = torch.amax(tensor, dim=(1, 2), keepdim=True)\n",
        "    return range_min + (range_max - range_min) * (tensor - min_val) / (max_val - min_val + 1e-6)\n",
        "\n",
        "def quantize(samples, bits=8, epsilon=0.01):\n",
        "    \"\"\"\n",
        "    Linearly quantize a signal in [0, 1] to a signal in [0, q_levels - 1].\n",
        "    \"\"\"\n",
        "    q_levels = 1 << bits\n",
        "    samples *= q_levels - epsilon\n",
        "    samples += epsilon / 2\n",
        "    return samples.long()\n",
        "\n",
        "def dequantize(samples, bits=8):\n",
        "    \"\"\"\n",
        "    Dequantize a signal in [0, q_levels - 1].\n",
        "    \"\"\"\n",
        "    q_levels = 1 << bits\n",
        "    return samples.float() / (q_levels / 2) - 1\n",
        "\n",
        "def mu_law_encode(audio, bits=8):\n",
        "    \"\"\"\n",
        "    Perform mu-law companding transformation.\n",
        "    \"\"\"\n",
        "    mu = torch.tensor((1 << bits) - 1)\n",
        "\n",
        "    # Audio must be min-max scaled between -1 and 1\n",
        "    audio = minmax_scale(audio, range_min=-1, range_max=1)\n",
        "\n",
        "    # Perform mu-law companding transformation.\n",
        "    numerator = torch.log1p(mu * torch.abs(audio + 1e-8))\n",
        "    denominator = torch.log1p(mu)\n",
        "    encoded = torch.sign(audio) * (numerator / denominator)\n",
        "\n",
        "    # Shift signal to [0, 1]\n",
        "    encoded = (encoded + 1) / 2\n",
        "\n",
        "    # Quantize signal to the specified number of levels.\n",
        "    return quantize(encoded, bits=bits)\n",
        "\n",
        "def mu_law_decode(encoded, bits=8):\n",
        "    \"\"\"\n",
        "    Perform inverse mu-law transformation.\n",
        "    \"\"\"\n",
        "    mu = (1 << bits) - 1\n",
        "    # Invert the quantization\n",
        "    x = dequantize(encoded, bits=bits)\n",
        "\n",
        "    # Invert the mu-law transformation\n",
        "    x = torch.sign(x) * ((1 + mu)**(torch.abs(x)) - 1) / mu\n",
        "\n",
        "    # Returned values in range [-1, 1]\n",
        "    return x\n",
        "\n",
        "def linear_encode(samples, bits=8):\n",
        "    \"\"\"\n",
        "    Perform scaling and linear quantization.\n",
        "    \"\"\"\n",
        "    samples = samples.clone()\n",
        "    samples = minmax_scale(samples)\n",
        "    return quantize(samples, bits=bits)\n",
        "\n",
        "def linear_decode(samples, bits=8):\n",
        "    \"\"\"\n",
        "    Invert the linear quantization.\n",
        "    \"\"\"\n",
        "    return dequantize(samples, bits=bits)\n",
        "\n",
        "def q_zero(bits=8):\n",
        "    \"\"\"\n",
        "    The quantized level of the 0.0 value.\n",
        "    \"\"\"\n",
        "    return 1 << (bits - 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MonoWavChunkDataset: 600 files, chunk_size=512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:00<00:00, 611860.54it/s]\n"
          ]
        }
      ],
      "source": [
        "# set seeds\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "dataset = MonoWavChunkDataset(data_dir='/graft3/datasets/pnlong/lnac/sashimi/data/musdb18stereo/train', chunk_size=512, bit_split=2, stereo_interleave=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/graft3/datasets/pnlong/lnac/sashimi/data/musdb18stereo/train/Secret Mountains - High Horse.0.wav\n",
            "torch.Size([2, 513]) 44100 torch.int16 tensor(-9225, dtype=torch.int16) tensor(7436, dtype=torch.int16)\n",
            "torch.int64 tensor(23543) tensor(40204)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/znovack/miniconda3/envs/lnac/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "item = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([134, 135, 136, 138, 139, 140, 142, 143, 144, 143, 144, 145, 145, 147,\n",
              "         148, 148, 148, 149, 149, 150, 150, 151, 152, 153, 154, 154, 154, 155,\n",
              "         155, 155, 156, 155, 155, 155, 154, 152, 152, 153, 151, 150, 150, 149,\n",
              "         149, 149, 148, 147, 146, 146, 145, 143, 142, 142, 141, 139, 137, 137,\n",
              "         137, 136, 134, 133, 132, 131, 132, 131, 130, 130, 131, 131, 131, 131,\n",
              "         133, 134, 135, 137, 139, 141, 143, 145, 146, 146, 147, 147, 146, 146,\n",
              "         147, 146, 146, 148, 148, 147, 148, 149, 150, 149, 149, 150, 150, 151,\n",
              "         151, 150, 149, 149, 149, 148, 147, 146, 145, 144, 143, 140, 139, 139,\n",
              "         138, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 137, 137, 138,\n",
              "         138, 139, 138, 138, 137, 136, 136, 135, 134, 134, 134, 134, 133, 132,\n",
              "         132, 132, 132, 132, 133, 134, 135, 134, 134, 134, 134, 133, 132, 131,\n",
              "         130, 130, 129, 129, 129, 128, 127, 127, 127, 126, 124, 124, 125, 124,\n",
              "         123, 123, 123, 124, 123, 123, 124, 123, 123, 123, 123, 121, 120, 120,\n",
              "         120, 117, 116, 117, 117, 118, 118, 118, 118, 119, 120, 120, 120, 120,\n",
              "         120, 119, 120, 121, 119, 116, 114, 114, 112, 111, 110, 109, 108, 107,\n",
              "         107, 107, 107, 108, 109, 109, 110, 111, 112, 113, 114, 115, 115, 116,\n",
              "         116, 117, 118, 118, 118, 119, 120, 120, 120, 121, 122, 122, 123, 124,\n",
              "         125, 125, 125, 124, 123, 122, 120, 119, 118, 117, 115, 114, 115, 115,\n",
              "         114, 115, 115, 115, 114, 115, 118, 118, 118, 120, 121, 121, 121, 121,\n",
              "         121, 121, 120, 119, 118, 115, 113, 112, 112, 111, 110, 108, 106, 106,\n",
              "         105, 103, 102, 103, 103, 103, 104, 105, 105, 106, 108, 109, 110, 112,\n",
              "         113, 115, 115, 115, 115, 115, 116, 116, 116, 116, 115, 114, 113, 113,\n",
              "         112, 111, 110, 110, 109, 108, 107, 105, 105, 106, 106, 107, 108, 109,\n",
              "         109, 109, 111, 111, 112, 115, 115, 115, 117, 120, 122, 124, 126, 129,\n",
              "         131, 132, 133, 135, 136, 136, 135, 135, 136, 136, 135, 134, 133, 133,\n",
              "         132, 132, 130, 129, 127, 127, 127, 126, 126, 126, 125, 123, 121, 119,\n",
              "         117, 115, 113, 112, 110, 109, 109, 109, 109, 110, 110, 109, 110, 111,\n",
              "         112, 113, 114, 115, 117, 117, 117, 117, 119, 120, 120, 121, 122, 123,\n",
              "         123, 123, 124, 125, 125, 126, 128, 129, 128, 128, 128, 128, 126, 125,\n",
              "         125, 125, 125, 124, 124, 124, 124, 124, 124, 124, 124, 124, 123, 123,\n",
              "         123, 123, 122, 122, 123, 123, 123, 124, 125, 125, 126, 127, 128, 128,\n",
              "         130, 130, 129, 129, 130, 132, 133, 133, 134, 134, 135, 136, 137, 137,\n",
              "         138, 138, 139, 140, 140, 140, 140, 138, 137, 136, 135, 134, 133, 131,\n",
              "         130, 129, 127, 127, 126, 126, 125, 123, 122, 121, 121, 120, 120, 121,\n",
              "         123, 125, 126, 126, 126, 126, 126, 127, 128, 129, 130, 132, 134, 137,\n",
              "         138, 140, 142, 144, 147, 149, 149, 149, 149, 149, 147, 147, 148, 146,\n",
              "         145, 145, 144, 142, 141, 141, 140, 138, 138]),\n",
              " tensor([ 53, 202,  16, 130, 123, 114, 133, 169,  25, 245, 129, 144, 242, 111,\n",
              "         191, 119, 232, 127, 141,   7, 110,  81, 191, 149,  48, 160, 184,   5,\n",
              "         133, 233,  15, 222, 127,  71, 119, 247, 243,  84, 118,  31, 133, 249,\n",
              "         112, 158, 166,  84, 215,  84,  46, 155, 148, 127, 189, 118, 152,  94,\n",
              "          99,  87, 219, 238, 237, 224,  12, 252, 188, 194, 219, 178,  33, 239,\n",
              "         108,  58,  29,  93, 245, 220, 209, 203, 103,  35,  24, 224, 158, 133,\n",
              "         208, 220,  48,   0, 159, 204,  97, 245,  82,  75,  59, 103, 245,  35,\n",
              "          17,  47, 121, 138, 154, 166,  14,  22, 159, 233,  17,  56,   5, 147,\n",
              "         171, 146,   8, 161,  96,  12, 179, 201,  83, 137, 192,  16, 178,  33,\n",
              "         207,  32, 186,  67,  52, 126, 181, 192,  66,   7,  39,  33, 182, 156,\n",
              "          50, 175, 197, 244, 252, 241,  18, 164, 128, 224, 155, 193, 251, 173,\n",
              "         181, 121, 153,  26,  69, 129, 183, 151,  66,  57, 201, 161,  49,  94,\n",
              "          89,  47, 153,  46, 211, 168,  69, 226, 107, 162,  16, 181, 193, 248,\n",
              "         126, 246, 172, 130, 255,  91, 207, 184, 211, 106,  39, 134, 139, 182,\n",
              "         102, 234, 116,   9, 173, 192, 198,  27, 248,  89,  10,  15,  30,  45,\n",
              "          40, 184, 242, 177, 125, 157, 107, 209, 198, 169, 185, 101, 148,  40,\n",
              "         238,  63,   2, 196, 195, 112, 113, 123, 144, 110,  75, 166, 103, 231,\n",
              "         202, 219, 124,  87,  41,  54, 218, 187, 248, 104,  86, 139,  44,  90,\n",
              "         223,  64, 253,  70,  81, 220,   0,  23, 174, 113,  11, 103, 203,  95,\n",
              "         110, 131, 189, 243, 114, 215, 169, 155,  35, 114,  40, 102, 220,  36,\n",
              "          23,  72, 169,  85, 148, 168, 116,  22,  88, 151,  57,   8,  73,  67,\n",
              "         215,  40, 242, 219, 159, 221, 159, 230, 114,  30,  76,  16, 165,  94,\n",
              "         147, 156, 183,  56, 158, 123,  42, 254, 198, 109, 224,  94,  86,  66,\n",
              "         137, 246,  47, 206, 180,  13, 219, 159, 235, 228, 133,  83, 215, 144,\n",
              "         145, 135, 224, 175, 125,   5,  90, 205, 238, 207, 121, 109, 234,  98,\n",
              "         178,   0, 254,  49, 165, 118, 129, 249, 179, 151, 153, 161, 141, 185,\n",
              "         144,  61, 168, 115, 191,  79,   8,  25,  82,  17,   0, 143, 176, 232,\n",
              "          63, 105, 238, 249,  85, 248,  78, 194, 207, 228, 200,  63,  80,  36,\n",
              "          54,  35, 110, 234, 246, 182, 234, 155, 144, 114, 253,  56, 152, 122,\n",
              "          93, 233, 193, 255, 225, 190,  49,  23,  82, 113,  99,  29, 168,  61,\n",
              "          76,  42, 102, 110,  80, 163, 213, 153,  45, 132, 130, 155,  18, 237,\n",
              "          28,  66, 175, 161, 211, 103,   1, 142, 128, 220,  89,  83,  28, 240,\n",
              "         135, 228, 183, 102, 180, 244,  81, 245, 248, 218, 133, 155, 106, 241,\n",
              "         166,  11, 201,  64, 163,  26,  54, 176, 163, 237,  33, 159, 158, 143,\n",
              "          78,  70, 157, 139,  49, 104, 144,  24,  69, 147, 242, 161, 231,  27,\n",
              "         236, 217,  95,  25,  22, 106, 127,  43, 238, 185, 230, 154,  19, 127,\n",
              "          34,  27,   7,  72, 205, 250, 226, 251,  29]),\n",
              " tensor([141, 145, 143, 146, 147, 148, 151, 152, 153, 152, 152, 154, 154, 154,\n",
              "         154, 153, 153, 154, 154, 154, 154, 154, 155, 155, 156, 155, 156, 156,\n",
              "         156, 156, 157, 157, 156, 156, 156, 156, 155, 154, 153, 152, 152, 153,\n",
              "         153, 151, 150, 149, 148, 147, 146, 143, 142, 142, 142, 141, 140, 142,\n",
              "         144, 144, 145, 147, 148, 148, 150, 151, 152, 152, 155, 156, 155, 155,\n",
              "         155, 154, 152, 152, 152, 152, 152, 152, 151, 149, 149, 150, 149, 147,\n",
              "         147, 146, 145, 145, 145, 145, 145, 145, 145, 144, 144, 143, 143, 142,\n",
              "         143, 142, 141, 141, 142, 141, 141, 140, 139, 139, 139, 137, 136, 137,\n",
              "         137, 135, 133, 132, 131, 130, 129, 129, 128, 128, 128, 128, 127, 128,\n",
              "         129, 129, 129, 128, 128, 129, 130, 129, 130, 131, 131, 130, 130, 130,\n",
              "         129, 129, 128, 127, 127, 128, 129, 128, 129, 130, 130, 129, 131, 132,\n",
              "         133, 134, 133, 134, 135, 134, 134, 135, 135, 135, 134, 134, 133, 132,\n",
              "         132, 131, 131, 131, 130, 129, 128, 127, 127, 127, 126, 124, 122, 122,\n",
              "         121, 118, 116, 116, 115, 114, 114, 113, 113, 112, 112, 111, 110, 110,\n",
              "         111, 111, 111, 112, 112, 111, 110, 110, 112, 112, 111, 111, 111, 111,\n",
              "         111, 110, 111, 112, 111, 111, 112, 112, 111, 111, 112, 112, 111, 111,\n",
              "         110, 109, 109, 108, 107, 106, 104, 104, 102, 101, 101, 100,  99,  98,\n",
              "          98,  98,  97,  95,  94,  93,  92,  91,  92,  92,  92,  93,  95,  97,\n",
              "          98, 100, 100, 100, 100, 100, 101, 100, 100, 101, 101, 101, 100, 100,\n",
              "         101, 102, 102, 103, 103, 102, 102, 103, 104, 105, 106, 106, 105, 105,\n",
              "         104, 103, 103, 103, 104, 104, 104, 103, 102, 101, 101, 101, 102, 102,\n",
              "         102, 103, 105, 107, 107, 108, 111, 113, 114, 115, 116, 117, 117, 118,\n",
              "         118, 118, 118, 119, 119, 119, 119, 119, 119, 118, 118, 118, 118, 118,\n",
              "         118, 117, 117, 117, 117, 117, 117, 117, 117, 117, 118, 118, 118, 118,\n",
              "         118, 118, 116, 116, 116, 115, 114, 114, 114, 114, 113, 113, 113, 112,\n",
              "         111, 111, 111, 111, 111, 112, 112, 112, 112, 113, 115, 116, 118, 119,\n",
              "         120, 121, 122, 123, 123, 124, 125, 126, 126, 127, 127, 127, 128, 129,\n",
              "         129, 129, 130, 130, 131, 131, 129, 129, 130, 130, 128, 127, 127, 126,\n",
              "         126, 125, 124, 124, 124, 125, 127, 128, 127, 128, 129, 129, 129, 129,\n",
              "         130, 133, 133, 133, 134, 136, 136, 134, 134, 136, 135, 135, 136, 137,\n",
              "         137, 137, 139, 140, 141, 143, 144, 145, 145, 146, 146, 147, 147, 148,\n",
              "         148, 147, 146, 145, 144, 143, 143, 142, 141, 140, 138, 137, 136, 136,\n",
              "         135, 135, 133, 132, 132, 133, 133, 133, 134, 135, 137, 138, 140, 141,\n",
              "         142, 143, 143, 144, 145, 147, 147, 146, 146, 146, 146, 146, 147, 148,\n",
              "         149, 151, 152, 150, 149, 148, 146, 146, 146, 147, 148, 148, 147, 146,\n",
              "         145, 144, 143, 142, 142, 143, 142, 142, 143, 143, 141, 141, 142, 142,\n",
              "         141, 141, 140, 139, 139, 139, 138, 137, 136]),\n",
              " tensor([173,  55, 145,  59, 153, 234, 181, 203,  81, 183, 193, 103,  76,  81,\n",
              "         189, 212, 194,  36,  86, 149,  72, 204, 165, 249,  65, 230,  33, 182,\n",
              "          35,  66,  12,   5, 163, 104, 143,  34,  23, 164, 227, 191, 170, 115,\n",
              "          86, 165, 126, 211,  62, 151, 165, 174,  74, 246, 195, 128, 240, 113,\n",
              "          44, 165, 203, 129,  41, 180,  72, 207,  10, 188,   5,  32, 133, 171,\n",
              "         227,  81, 124,  88, 138,   1,  41, 124,  25,  99, 197, 149,  47, 210,\n",
              "         249, 251, 155, 168, 151,  97, 123, 101,  94, 213,  19, 220,  58, 206,\n",
              "          38, 165, 219, 252,  29, 228,  59,  20, 177, 225,   1, 127, 244, 131,\n",
              "          64,  20,  64, 184, 200, 177, 253,  33, 138, 128, 201, 164, 242,  72,\n",
              "          14,   7,  35, 249, 193, 193,  95, 251,  81,  53,  85, 250, 224,  37,\n",
              "          20,  61, 246, 126, 135, 245, 109, 252,  96,  75,   1, 203,   4, 126,\n",
              "         215,  59, 236, 193,  22,  71, 121,  84, 214, 140, 154,  15, 128, 207,\n",
              "          84, 218, 233, 121,  64, 153, 119, 134, 244, 140,  88, 205, 189,  81,\n",
              "         172, 102,  71, 102, 189, 147,  69, 254,  56, 208, 194, 252, 241, 207,\n",
              "          69,  44,  13,  67, 191,  25,  40, 255,   6,  45, 143,  86, 126, 105,\n",
              "          51, 254, 165,  80, 162, 151, 146, 143, 238, 243,  92,   4, 112,  92,\n",
              "         132, 131,  44, 101, 154, 127, 250,  44, 235, 184, 151, 179,  78,  77,\n",
              "          41, 210, 135,  87, 216, 233,  96, 247, 109, 226, 176, 149, 249, 155,\n",
              "         232,  47, 198, 231,  98, 143,  37, 134, 144,  52,  37,  51, 145,  77,\n",
              "         203, 124, 146, 139, 159, 166,  92,  29, 121, 244, 191, 119, 234,  60,\n",
              "          49, 158, 162, 223,  71, 158, 164, 142,  10, 193, 215, 201,  16,  44,\n",
              "         173, 241, 134,  21, 212, 250, 114,  95, 140, 159,  84,  18, 188,  55,\n",
              "         102,  30, 182, 204, 203, 127,  92,  20,   0, 246, 207, 135, 104, 183,\n",
              "          69,  68,  36,  74, 119, 160,  50,  69, 216, 234,  76, 144,  20,  98,\n",
              "         242,  23, 246, 167,  86, 123, 142,  51, 103,  52,  84,  44, 176, 216,\n",
              "          95,  85, 143,  19,  69,  24,  95,  73, 160, 181,  74, 231, 109, 192,\n",
              "         151, 101, 156,  95, 118,  65, 142,  38, 189, 124,  74,  69, 131, 117,\n",
              "         162, 248,  46, 114,  42,  18, 240, 199, 208, 118, 134, 209, 221, 213,\n",
              "          53, 222, 184, 120, 217,  79,  18,  12, 134,  34,   8,  41, 113, 183,\n",
              "         237,  27, 206, 145, 169,  54,   4, 191, 252,  17, 219, 195, 222, 100,\n",
              "          49, 198,  38, 144, 230,  30,  55,  74, 238,  43, 206, 142, 247,  93,\n",
              "          78, 172, 244, 157,  37, 213, 150,  80,  20,  64, 254, 105,  97,  17,\n",
              "         216,  21, 178, 175, 239, 125, 103,  92,  14, 106,  46, 242, 119, 198,\n",
              "         244, 211, 249,  49, 205, 121,  72,  62,  33, 210, 221,  82,  27, 187,\n",
              "         219,  76,   1, 161,   5,   1, 252,  83, 155, 247, 196,  30, 151, 235,\n",
              "         118, 112, 146, 180, 241, 103, 210,  92,  17,  13, 151, 109,  97,   5,\n",
              "         100,  31,  25,  68,  82,  65, 148, 133, 172]))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/znovack/miniconda3/envs/lnac/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# now load it directly as float and see if we get the same thing\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "pth = \"/graft3/datasets/pnlong/lnac/sashimi/data/musdb18stereo/train/Secret Mountains - High Horse.0.wav\"\n",
        "ref_wav, sr = torchaudio.load(pth, frame_offset=0, num_frames=513, backend=\"soundfile\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_wav_24b = quantize_unsigned_pcm(ref_wav.numpy(), 24)\n",
        "ref_wav_16b = quantize_unsigned_pcm(ref_wav.numpy(), 16)\n",
        "ref_wav_8b = quantize_unsigned_pcm(ref_wav.numpy(), 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2592294/1116458204.py:1: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  item[3] - ref_wav_16b_8lsb[0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item[3] - ref_wav_16b_8lsb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_wav_24b_16msb = msb(ref_wav_24b, 24, 16)\n",
        "ref_wav_24b_8msb = msb(ref_wav_24b, 24, 8)\n",
        "ref_wav_16b_8msb = msb(ref_wav_16b, 16, 8)\n",
        "ref_wav_16b_8lsb = lsb(ref_wav_16b, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[173,  55, 145, ..., 148, 133, 172],\n",
              "       [ 53, 202,  16, ..., 226, 251,  29]], shape=(2, 513), dtype=uint64)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ref_wav_16b_8lsb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.load(\"/graft3/datasets/haven/sketch2music/0000000010000.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 1937])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import stable_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lnac",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
